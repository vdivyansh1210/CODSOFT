{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Data Loading and Cleaning ---\n",
        "\n",
        "# Define column names based on the file structure\n",
        "TRAIN_COLUMNS = ['ID', 'TITLE', 'GENRE', 'DESCRIPTION']\n",
        "TEST_COLUMNS = ['ID', 'TITLE', 'DESCRIPTION']\n",
        "SKIP_COLUMNS = ['ID', 'TITLE']\n",
        "\n",
        "# Function to safely load and parse the raw data files with ' ::: ' delimiter\n",
        "def load_and_clean_data(file_path, cols):\n",
        "    try:\n",
        "        # Read the file line by line and split on ' ::: '\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            # We strip each line before splitting to ensure clean parsing\n",
        "            data = [line.strip().split(' ::: ') for line in f if line.strip()]\n",
        "\n",
        "        # Filter for lines with the correct number of columns\n",
        "        if len(data) > 0 and len(data[0]) == len(cols):\n",
        "            df = pd.DataFrame(data, columns=cols)\n",
        "        else:\n",
        "            print(f\"Warning: Data in {file_path} did not match expected column count ({len(cols)}).\")\n",
        "            return pd.DataFrame(columns=cols)\n",
        "\n",
        "        # Remove metadata columns not used for prediction\n",
        "        df = df.drop(columns=SKIP_COLUMNS, errors='ignore')\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}. Please check your file names.\")\n",
        "        return pd.DataFrame(columns=cols)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading {file_path}: {e}\")\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "\n",
        "# --- Load Datasets ---\n",
        "train_df = load_and_clean_data('train_data.txt', TRAIN_COLUMNS)\n",
        "test_df = load_and_clean_data('test_data.txt', TEST_COLUMNS)\n",
        "solution_df = load_and_clean_data('test_data_solution.txt', TRAIN_COLUMNS)\n",
        "\n",
        "\n",
        "# Check if dataframes were loaded successfully\n",
        "if train_df.empty or test_df.empty or solution_df.empty:\n",
        "    print(\"Aborting analysis due to missing or improperly loaded data files.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. Text Preprocessing Function ---\n",
        "def clean_text(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers and special characters (keeping only letters and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to plot summaries\n",
        "X_train_raw = train_df['DESCRIPTION'].apply(clean_text)\n",
        "X_test_raw = test_df['DESCRIPTION'].apply(clean_text)\n",
        "\n",
        "# Extract target labels\n",
        "y_train_raw = train_df['GENRE']\n",
        "y_true = solution_df['GENRE']\n",
        "\n",
        "\n",
        "# --- 3. Label Encoding (for target variable Y) ---\n",
        "# Convert string labels (genres) into numerical form for the classifier\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train_raw)\n",
        "y_true_encoded = label_encoder.transform(y_true)\n",
        "\n",
        "\n",
        "# --- 4. Feature Engineering (TF-IDF) ---\n",
        "# Fit the TfidfVectorizer only on the training data to prevent data leakage.\n",
        "# Common configuration: max_features for dimensionality reduction, sublinear_tf for scaling.\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    max_features=25000, # Using a reasonable number of top words\n",
        "    sublinear_tf=True,\n",
        "    ngram_range=(1, 2) # Include bigrams for better context\n",
        ")\n",
        "\n",
        "print(\"--- Data Processing ---\")\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
        "\n",
        "print(f\"Training Samples: {X_train_tfidf.shape[0]}, Features: {X_train_tfidf.shape[1]}\")\n",
        "print(f\"Test Samples: {X_test_tfidf.shape[0]}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "\n",
        "# --- 5. Model Training (Logistic Regression) ---\n",
        "# Logistic Regression is fast and effective for this type of high-dimensional sparse data.\n",
        "# Using 'saga' solver for better handling of multinomial loss on sparse data.\n",
        "model = LogisticRegression(\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    C=5.0, # Regularization strength (higher C = less regularization)\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Training Logistic Regression Model...\")\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "print(\"Training Complete.\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "\n",
        "# --- 6. Prediction and Evaluation ---\n",
        "y_pred_encoded = model.predict(X_test_tfidf)\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "accuracy = accuracy_score(y_true_encoded, y_pred_encoded)\n",
        "\n",
        "print(\"--- Model Evaluation ---\")\n",
        "print(f\"Accuracy on Test Solution Data: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "\n",
        "# Generate and print the classification report\n",
        "report = classification_report(\n",
        "    y_true_encoded,\n",
        "    y_pred_encoded,\n",
        "    target_names=label_encoder.classes_,\n",
        "    zero_division=0\n",
        ")\n",
        "print(report)\n",
        "\n",
        "# Optional: Save predictions to a file\n",
        "prediction_df = pd.DataFrame({\n",
        "    'Predicted_Genre': y_pred_labels,\n",
        "    'True_Genre': y_true\n",
        "})\n",
        "# prediction_df.to_csv('predictions.csv', index=False)\n",
        "# print(\"\\nPredictions saved to predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYcpH_oRL3O5",
        "outputId": "bd2c6108-94ae-46c5-d408-d122d39f25f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Processing ---\n",
            "Training Samples: 54214, Features: 25000\n",
            "Test Samples: 54200\n",
            "-----------------------------------\n",
            "Training Logistic Regression Model...\n",
            "Training Complete.\n",
            "-----------------------------------\n",
            "--- Model Evaluation ---\n",
            "Accuracy on Test Solution Data: 0.5950\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      action       0.47      0.35      0.40      1314\n",
            "       adult       0.67      0.37      0.48       590\n",
            "   adventure       0.55      0.22      0.32       775\n",
            "   animation       0.47      0.13      0.20       498\n",
            "   biography       0.00      0.00      0.00       264\n",
            "      comedy       0.55      0.60      0.57      7446\n",
            "       crime       0.25      0.06      0.09       505\n",
            " documentary       0.70      0.83      0.76     13096\n",
            "       drama       0.56      0.74      0.64     13612\n",
            "      family       0.45      0.14      0.21       783\n",
            "     fantasy       0.43      0.10      0.16       322\n",
            "   game-show       0.88      0.59      0.70       193\n",
            "     history       0.44      0.02      0.03       243\n",
            "      horror       0.66      0.61      0.63      2204\n",
            "       music       0.67      0.51      0.58       731\n",
            "     musical       0.28      0.05      0.08       276\n",
            "     mystery       0.31      0.03      0.05       318\n",
            "        news       0.62      0.12      0.20       181\n",
            "  reality-tv       0.51      0.26      0.35       883\n",
            "     romance       0.40      0.08      0.13       672\n",
            "      sci-fi       0.56      0.32      0.41       646\n",
            "       short       0.44      0.38      0.41      5072\n",
            "       sport       0.62      0.36      0.46       431\n",
            "   talk-show       0.61      0.27      0.37       391\n",
            "    thriller       0.35      0.18      0.24      1590\n",
            "         war       0.54      0.11      0.19       132\n",
            "     western       0.91      0.78      0.84      1032\n",
            "\n",
            "    accuracy                           0.59     54200\n",
            "   macro avg       0.52      0.30      0.35     54200\n",
            "weighted avg       0.57      0.59      0.57     54200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}